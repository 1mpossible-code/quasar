{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4fdb16",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Classical image recognition using a convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f012863",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Load training data and test data\n",
    "train_dataset = \"./dataset/recycled_32_train.npz\"\n",
    "train_data = np.load(train_dataset)\n",
    "\n",
    "test_dataset = \"./dataset/recycled_32_test.npz\"\n",
    "test_data = np.load(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577193f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The \"x\" of the dataset contains a two dimension numpy array of uint8, where each\n",
    "row contains a 32x32 coloured image. The picture follows the \"channel first\" rule.\n",
    "\n",
    "The \"y\" of the dataset contains a one dimension numpy array of uint8, where each\n",
    "value indicates the label of corresponding x item.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d67072e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Extract data from npz file\n",
    "\n",
    "# Train data\n",
    "train_images = train_data[\"x\"]\n",
    "train_labels = train_data[\"y\"]\n",
    "\n",
    "# Test data\n",
    "test_images = test_data[\"x\"]\n",
    "test_labels = test_data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3b1637b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Preprocess the data\n",
    "\n",
    "# Normalize the data\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# Reshape the data\n",
    "train_images = train_images.reshape(train_images.shape[0], 32, 32, 3)\n",
    "\n",
    "test_images = test_images.reshape(test_images.shape[0], 32, 32, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef47840a",
   "metadata": {},
   "source": [
    "We use only 3 layers to simplify the model and make the fair competition with the ability to test the model, run in different scenarios and cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4064e0e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Build the model\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9a8173",
   "metadata": {},
   "source": [
    "We still use Adam optimizer, and the loss function is still cross entropy, because we want to acheive the best performance within the simplest model layer set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796b29a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb18b8a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "313/313 - 1s - loss: 1.3604 - accuracy: 0.4414 - 997ms/epoch - 3ms/step\n",
      "Epoch 2/50\n",
      "313/313 - 1s - loss: 1.1259 - accuracy: 0.5455 - 1s/epoch - 4ms/step\n",
      "Epoch 3/50\n",
      "313/313 - 1s - loss: 1.0731 - accuracy: 0.5781 - 1s/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "313/313 - 1s - loss: 1.0428 - accuracy: 0.5872 - 738ms/epoch - 2ms/step\n",
      "Epoch 5/50\n",
      "313/313 - 1s - loss: 1.0015 - accuracy: 0.6066 - 1s/epoch - 3ms/step\n",
      "Epoch 6/50\n",
      "313/313 - 1s - loss: 0.9572 - accuracy: 0.6145 - 729ms/epoch - 2ms/step\n",
      "Epoch 7/50\n",
      "313/313 - 1s - loss: 0.9228 - accuracy: 0.6357 - 710ms/epoch - 2ms/step\n",
      "Epoch 8/50\n",
      "313/313 - 1s - loss: 0.8851 - accuracy: 0.6508 - 710ms/epoch - 2ms/step\n",
      "Epoch 9/50\n",
      "313/313 - 1s - loss: 0.8567 - accuracy: 0.6596 - 706ms/epoch - 2ms/step\n",
      "Epoch 10/50\n",
      "313/313 - 1s - loss: 0.8321 - accuracy: 0.6789 - 732ms/epoch - 2ms/step\n",
      "Epoch 11/50\n",
      "313/313 - 1s - loss: 0.8057 - accuracy: 0.6882 - 709ms/epoch - 2ms/step\n",
      "Epoch 12/50\n",
      "313/313 - 1s - loss: 0.7858 - accuracy: 0.6965 - 723ms/epoch - 2ms/step\n",
      "Epoch 13/50\n",
      "313/313 - 1s - loss: 0.7513 - accuracy: 0.7007 - 946ms/epoch - 3ms/step\n",
      "Epoch 14/50\n",
      "313/313 - 1s - loss: 0.7429 - accuracy: 0.7115 - 1s/epoch - 3ms/step\n",
      "Epoch 15/50\n",
      "313/313 - 1s - loss: 0.7369 - accuracy: 0.7096 - 789ms/epoch - 3ms/step\n",
      "Epoch 16/50\n",
      "313/313 - 1s - loss: 0.7205 - accuracy: 0.7176 - 1s/epoch - 4ms/step\n",
      "Epoch 17/50\n",
      "313/313 - 1s - loss: 0.7289 - accuracy: 0.7105 - 1s/epoch - 4ms/step\n",
      "Epoch 18/50\n",
      "313/313 - 1s - loss: 0.7144 - accuracy: 0.7151 - 703ms/epoch - 2ms/step\n",
      "Epoch 19/50\n",
      "313/313 - 1s - loss: 0.6895 - accuracy: 0.7292 - 727ms/epoch - 2ms/step\n",
      "Epoch 20/50\n",
      "313/313 - 1s - loss: 0.6818 - accuracy: 0.7299 - 710ms/epoch - 2ms/step\n",
      "Epoch 21/50\n",
      "313/313 - 1s - loss: 0.6692 - accuracy: 0.7399 - 707ms/epoch - 2ms/step\n",
      "Epoch 22/50\n",
      "313/313 - 1s - loss: 0.6572 - accuracy: 0.7397 - 725ms/epoch - 2ms/step\n",
      "Epoch 23/50\n",
      "313/313 - 1s - loss: 0.6598 - accuracy: 0.7400 - 702ms/epoch - 2ms/step\n",
      "Epoch 24/50\n",
      "313/313 - 1s - loss: 0.6431 - accuracy: 0.7401 - 770ms/epoch - 2ms/step\n",
      "Epoch 25/50\n",
      "313/313 - 1s - loss: 0.6331 - accuracy: 0.7501 - 727ms/epoch - 2ms/step\n",
      "Epoch 26/50\n",
      "313/313 - 1s - loss: 0.6535 - accuracy: 0.7400 - 743ms/epoch - 2ms/step\n",
      "Epoch 27/50\n",
      "313/313 - 1s - loss: 0.6526 - accuracy: 0.7429 - 699ms/epoch - 2ms/step\n",
      "Epoch 28/50\n",
      "313/313 - 1s - loss: 0.6242 - accuracy: 0.7523 - 748ms/epoch - 2ms/step\n",
      "Epoch 29/50\n",
      "313/313 - 1s - loss: 0.6024 - accuracy: 0.7600 - 761ms/epoch - 2ms/step\n",
      "Epoch 30/50\n",
      "313/313 - 1s - loss: 0.6058 - accuracy: 0.7595 - 705ms/epoch - 2ms/step\n",
      "Epoch 31/50\n",
      "313/313 - 1s - loss: 0.6042 - accuracy: 0.7635 - 702ms/epoch - 2ms/step\n",
      "Epoch 32/50\n",
      "313/313 - 1s - loss: 0.6138 - accuracy: 0.7592 - 701ms/epoch - 2ms/step\n",
      "Epoch 33/50\n",
      "313/313 - 1s - loss: 0.6038 - accuracy: 0.7580 - 708ms/epoch - 2ms/step\n",
      "Epoch 34/50\n",
      "313/313 - 1s - loss: 0.5903 - accuracy: 0.7613 - 700ms/epoch - 2ms/step\n",
      "Epoch 35/50\n",
      "313/313 - 1s - loss: 0.5857 - accuracy: 0.7708 - 775ms/epoch - 2ms/step\n",
      "Epoch 36/50\n",
      "313/313 - 1s - loss: 0.5684 - accuracy: 0.7752 - 696ms/epoch - 2ms/step\n",
      "Epoch 37/50\n",
      "313/313 - 1s - loss: 0.5780 - accuracy: 0.7692 - 707ms/epoch - 2ms/step\n",
      "Epoch 38/50\n",
      "313/313 - 1s - loss: 0.5666 - accuracy: 0.7765 - 706ms/epoch - 2ms/step\n",
      "Epoch 39/50\n",
      "313/313 - 1s - loss: 0.5657 - accuracy: 0.7779 - 700ms/epoch - 2ms/step\n",
      "Epoch 40/50\n",
      "313/313 - 1s - loss: 0.5610 - accuracy: 0.7788 - 702ms/epoch - 2ms/step\n",
      "Epoch 41/50\n",
      "313/313 - 1s - loss: 0.5736 - accuracy: 0.7661 - 708ms/epoch - 2ms/step\n",
      "Epoch 42/50\n",
      "313/313 - 1s - loss: 0.5388 - accuracy: 0.7894 - 705ms/epoch - 2ms/step\n",
      "Epoch 43/50\n",
      "313/313 - 1s - loss: 0.5411 - accuracy: 0.7906 - 702ms/epoch - 2ms/step\n",
      "Epoch 44/50\n",
      "313/313 - 1s - loss: 0.5325 - accuracy: 0.7910 - 704ms/epoch - 2ms/step\n",
      "Epoch 45/50\n",
      "313/313 - 1s - loss: 0.5309 - accuracy: 0.7916 - 708ms/epoch - 2ms/step\n",
      "Epoch 46/50\n",
      "313/313 - 1s - loss: 0.5271 - accuracy: 0.7967 - 766ms/epoch - 2ms/step\n",
      "Epoch 47/50\n",
      "313/313 - 1s - loss: 0.5321 - accuracy: 0.7867 - 714ms/epoch - 2ms/step\n",
      "Epoch 48/50\n",
      "313/313 - 1s - loss: 0.5183 - accuracy: 0.7956 - 811ms/epoch - 3ms/step\n",
      "Epoch 49/50\n",
      "313/313 - 1s - loss: 0.5369 - accuracy: 0.7875 - 712ms/epoch - 2ms/step\n",
      "Epoch 50/50\n",
      "313/313 - 1s - loss: 0.5114 - accuracy: 0.7982 - 702ms/epoch - 2ms/step\n",
      "Training time: 39.41671180725098 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=50, verbose=2)\n",
    "\n",
    "end = time.time() - start\n",
    "\n",
    "print(\"Training time:\", end, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c71cbe",
   "metadata": {},
   "source": [
    "We time the training process and the testing process, and we also record the accuracy of the model, so we can use this data in comparison with the other models and then make the conclusion and analysis with visual component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc610056",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 - 0s - loss: 0.6886 - accuracy: 0.7347 - 152ms/epoch - 3ms/step\n",
      "\n",
      "Test accuracy: 0.734666645526886\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "print(\"\\nTest accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "359d859e-affd-4f28-9c8e-e52db0697901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/max/learning/tasks/venv/lib/python3.9/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "\n",
    "model.save(\"./model/classical_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09737d59",
   "metadata": {},
   "source": [
    "After the model is saved, we can use the model to predict the image, and we can also use the model to predict the image in the test dataset, and then we can compare the result with the real label to see the accuracy of the model.\n",
    "\n",
    "The final model will be in releases of the repository, so it can be freely accessed and tested.\n",
    "\n",
    "*All the training and testing processes are done using Macbook Air 2020 M1 16GB RAM.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
